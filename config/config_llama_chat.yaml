pretrained_model_name_or_path: meta-llama/Llama-2-7b-chat-hf

train_batch_size: 16
eval_batch_size: 16
gradient_accumulation_steps: 1
learning_rate: 0.0001
weight_decay: 0
num_epochs: 10
generation_max_length: 200
max_seq_length: 512
seed: 42

lora:
  r: 8
  lora_alpha: 32
  lora_dropout: 0.05
  bias: none
  task_type: CAUSAL_LM
  inference_mode: false
